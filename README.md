# 中文生成式预训练模型

NLU的预训练模型大家应该见过不少了，NLG的预训练资源还比较少。这里汇总一些中文的生成式预训练模型，给出bert4keras下的加载方式。

## GPT

以GPT为代表的单向语言模型预训练。

### GPT2-ML

- 链接：https://github.com/imcaspar/gpt2-ml
- 大小：15亿参数，体积5.3G
- 说明：基于BERT代码修改，跟最大的英文版GPT2大小一致，目前开放了两个版本，详情请查看项目说明。
- 使用：[basic_language_model_gpt2_ml.py](https://github.com/bojone/bert4keras/blob/master/examples/basic_language_model_gpt2_ml.py)

## 交流

QQ交流群：67729435，微信群请加机器人微信号spaces_ac_cn
